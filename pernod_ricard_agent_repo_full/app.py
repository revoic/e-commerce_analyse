# E-Commerce Intelligence Tool - MVP UI
# Version 2.0 - Multi-Company Support

import os
import sys
import streamlit as st
from datetime import datetime

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Import scraper
try:
    from core.scraper import CompanyIntelligenceScraper
    SCRAPER_AVAILABLE = True
except Exception as e:
    SCRAPER_AVAILABLE = False
    SCRAPER_ERROR = str(e)

# Page config
st.set_page_config(
    page_title="E-Commerce Intelligence Tool",
    page_icon="üõí",
    layout="wide"
)

# Header
st.title("üõí E-Commerce Intelligence Tool")
st.caption("AI-powered market intelligence for EU/DE E-Commerce")

# Sidebar
with st.sidebar:
    st.header("üîç Company Analysis")
    
    company_name = st.text_input(
        "Company Name",
        placeholder="e.g., Coca-Cola, Zalando, Unilever",
        help="Enter any company name to analyze"
    )
    
    with st.expander("‚öôÔ∏è Advanced Options"):
        lookback_days = st.slider("Lookback (days)", 7, 30, 14)
        max_per_source = st.slider("Max sources per type", 5, 20, 10)
        
        domain = st.text_input("Domain (optional)", placeholder="example.com")
        newsroom_url = st.text_input("Newsroom URL (optional)")
        linkedin_url = st.text_input("LinkedIn URL (optional)")
    
    analyze_btn = st.button(
        "üöÄ Discover Sources",
        type="primary",
        disabled=not company_name,
        use_container_width=True
    )
    
    st.divider()
    
    st.info("""
    **üá™üá∫ EU/DE Market Focus**
    
    This tool specializes in:
    ‚Ä¢ European markets (14 countries)
    ‚Ä¢ E-Commerce & Retail Media
    ‚Ä¢ Marketplaces (Amazon, Zalando)
    ‚Ä¢ D2C & Digital Commerce
    """)
    
    # Show model info
    model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    st.caption(f"ü§ñ Model: {model}")

# Main content
if not SCRAPER_AVAILABLE:
    st.error(f"""
    ‚ùå **Scraper not available**
    
    Error: {SCRAPER_ERROR}
    
    Please check that all dependencies are installed:
    ```
    pip install -r requirements.txt
    ```
    """)
    st.stop()

# Welcome screen
if not company_name:
    st.markdown("""
    ## üëã Welcome!
    
    This tool helps you discover and analyze E-Commerce intelligence for any company.
    
    ### üöÄ How it works:
    
    1. **Enter a company name** in the sidebar
    2. **Click "Discover Sources"** to start
    3. **View discovered sources** from multiple channels
    
    ### üìä What you get:
    
    - üì∞ **Google News** (14 EU editions)
    - üíº **LinkedIn** posts and updates
    - üè¢ **Company Newsroom** articles
    - üõí **E-Commerce** focused content
    
    ### üîí Privacy:
    
    - No data is stored without your consent
    - API calls are made via your OpenAI key
    - All sources are public information
    
    ---
    
    **üëà Enter a company name to get started!**
    """)
    
    # Show example companies
    st.subheader("üí° Try these examples:")
    cols = st.columns(4)
    example_companies = ["Coca-Cola", "Zalando", "Unilever", "LVMH"]
    for col, company in zip(cols, example_companies):
        if col.button(company, use_container_width=True):
            st.rerun()
    
    st.stop()

# Run analysis
if analyze_btn or "last_analysis" in st.session_state:
    
    # Build config
    config = {
        "lookback_days": lookback_days,
        "max_per_source": max_per_source
    }
    
    if domain:
        config["domain"] = domain
    if newsroom_url:
        config["newsroom_url"] = newsroom_url
    if linkedin_url:
        config["linkedin_url"] = linkedin_url
    
    # Store in session
    if analyze_btn:
        st.session_state["last_analysis"] = {
            "company": company_name,
            "config": config,
            "timestamp": datetime.now()
        }
    
    # Get from session
    analysis = st.session_state.get("last_analysis")
    if not analysis:
        st.stop()
    
    company_name = analysis["company"]
    config = analysis["config"]
    
    st.header(f"üìä Analysis: {company_name}")
    
    # Progress
    with st.spinner("üîç Discovering sources..."):
        try:
            import traceback
            
            st.info("Creating scraper...")
            scraper = CompanyIntelligenceScraper(company_name, config)
            
            st.info("Discovering sources...")
            sources = scraper.discover_all_sources()
            
            st.info("Getting stats...")
            stats = scraper.get_stats()
            
            st.info(f"Found {len(sources)} sources!")
            
        except Exception as e:
            st.error(f"""
            ‚ùå **Discovery failed**
            
            **Error:** {str(e)}
            
            **Type:** {type(e).__name__}
            
            **Traceback:**
            ```
            {traceback.format_exc()}
            ```
            
            Please check:
            - OpenAI API Key is set correctly
            - Internet connection is available
            - No rate limits reached
            """)
            st.stop()

    # Success message
    if len(sources) == 0:
        st.warning(f"""
        ‚ö†Ô∏è **No sources found for {company_name}**
        
        This could mean:
        - Company name is not well-known or misspelled
        - No recent news in the lookback period ({lookback_days} days)
        - Try adjusting your settings or company name
        
        **Stats:**
        - Google News: {stats.get('google_news', 0)}
        - LinkedIn: {stats.get('linkedin', 0)}
        - Newsroom: {stats.get('newsroom', 0)}
        """)
        
        if stats.get("errors"):
            with st.expander("üêõ Errors encountered"):
                for err in stats["errors"]:
                    st.error(err)
        
        st.stop()
    
    st.success(f"‚úÖ Discovered {len(sources)} sources!")
    
    # KPIs
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        st.metric("Total Sources", len(sources))
    
    with col2:
        eu_count = sum(1 for s in sources if "gnews:de:" in s.get("source", "") or "gnews:at:" in s.get("source", ""))
        st.metric("EU Sources", eu_count)
    
    with col3:
        st.metric("Google News", stats.get("google_news", 0))
    
    with col4:
        st.metric("LinkedIn", stats.get("linkedin", 0))
    
    with col5:
        st.metric("Newsroom", stats.get("newsroom", 0))
    
    # Errors
    if stats.get("errors"):
        with st.expander(f"‚ö†Ô∏è Warnings ({len(stats['errors'])})"):
            for err in stats["errors"]:
                st.warning(err)
    
    st.divider()

    # Sources table
    st.subheader("üìö Discovered Sources")
    
    if sources:
        # Filters
        col1, col2 = st.columns([3, 1])
        
        with col1:
            search = st.text_input("üîç Search", placeholder="Search in titles or URLs...")
        
        with col2:
            source_filter = st.multiselect(
                "Filter by type",
                options=list(set(s.get("source", "unknown") for s in sources)),
                default=[]
            )
        
        # Apply filters
        filtered = sources
        
        if search:
            search_lower = search.lower()
            filtered = [
                s for s in filtered
                if search_lower in s.get("title", "").lower() or search_lower in s.get("url", "").lower()
            ]
        
        if source_filter:
            filtered = [s for s in filtered if s.get("source") in source_filter]
        
        st.caption(f"Showing {len(filtered)} of {len(sources)} sources")
        
        # Display sources
        for i, source in enumerate(filtered, 1):
            with st.expander(f"{i}. {source.get('title', 'Untitled')}"):
                st.markdown(f"**URL:** {source.get('url', 'N/A')}")
                st.markdown(f"**Source:** `{source.get('source', 'unknown')}`")
                
                if source.get('published_at'):
                    st.markdown(f"**Published:** {source['published_at']}")
                
                st.link_button("Open Source", source.get('url', '#'))
        
        # Export
        st.divider()
        
        import json
        sources_json = json.dumps(filtered, indent=2, ensure_ascii=False)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.download_button(
                "üì• Download as JSON",
                sources_json.encode('utf-8'),
                f"{company_name}_sources.json",
                "application/json",
                use_container_width=True
            )
        
        with col2:
            # Simple CSV
            csv_lines = ["Title,URL,Source,Published\n"]
            for s in filtered:
                title = s.get('title', '').replace(',', ';')
                url = s.get('url', '')
                source = s.get('source', '')
                published = s.get('published_at', '')
                csv_lines.append(f'"{title}","{url}","{source}","{published}"\n')
            csv_data = ''.join(csv_lines)
            
            st.download_button(
                "üì• Download as CSV",
                csv_data.encode('utf-8'),
                f"{company_name}_sources.csv",
                "text/csv",
                use_container_width=True
            )
    else:
        st.info("No sources found. Try adjusting your filters or lookback period.")

# Footer
st.divider()
st.caption(f"""
E-Commerce Intelligence Tool v2.0 | 
Powered by OpenAI {os.getenv("OPENAI_MODEL", "gpt-4o-mini")} | 
Last updated: {datetime.now().strftime("%Y-%m-%d %H:%M")}
""")
